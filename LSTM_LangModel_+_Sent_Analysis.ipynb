{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM LangModel + Sent Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahnazshamissa/Python/blob/main/LSTM_LangModel_%2B_Sent_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5zwGMzVD0pF"
      },
      "source": [
        "# RNN / BiLSTM and Word Vectors\n",
        "\n",
        "Let's re-use the example you've had with Matthias Lechner, a few weeks back.\n",
        "\n",
        "We'll load the frankenstein book, and convert it into semantic representation through word vectors.\n",
        "\n",
        "We then train a language model, using LSTM, through these vectors.\n",
        "Later, please change the data source from \"frankenstein.txt\" to \"dracula.txt\", and observe the result. \n",
        "\n",
        "### How are we going to do it?\n",
        "\n",
        "We will define our data, as such, that for every word we use as an input for the model (X = Wn), the next word would be the output (Y = Wn+1)\n",
        "\n",
        "The words in the output, Y, will be represented as a one-hot-vector. \n",
        "\n",
        "**Q: What is the size of this Vector?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NTgD9mVh-gi",
        "outputId": "439ad739-9da2-42f9-cf0b-0207e7d955b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install bpemb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bpemb\n",
            "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bpemb) (1.18.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (from bpemb) (3.6.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bpemb) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb) (2020.6.20)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.2 sentencepiece-0.1.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrUhaljjLeco"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import math\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, tensor\n",
        "\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "from bpemb import BPEmb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYT3W-Rekse4"
      },
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSl2MJaLD-au"
      },
      "source": [
        "# Word Vectors - BPEmb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spXr-wu8D8P2"
      },
      "source": [
        "Let's convert the text into vectors using **BPE**.\n",
        "\n",
        "Byte Pair Encoding (BPE) is used to encode the input sequences. BPE was originally proposed as a data compression algorithm in 1990s and then was adopted to solve the open-vocabulary issue in machine translation, as we can easily run into rare and unknown words when translating into a new language. Motivated by the intuition that rare and unknown words can often be decomposed into multiple subwords, BPE finds the best word segmentation by iteratively and greedily merging frequent pairs of characters.\n",
        "\n",
        "We will use the BPE package, which is called [BPEmb](https://nlp.h-its.org/bpemb/). It encodes words to vectors by dividing each word to its to sub-words, pieces of words, made of characters which often appear together.\n",
        "\n",
        "It is based on the paper: Neural Machine Translation of Rare Words with Subword Units - https://arxiv.org/abs/1508.07909\n",
        "\n",
        "Q: What is the name of the Linguistic level that deals with character/letter-level? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgKbDV-5qAXc",
        "outputId": "60af7586-6bfc-46d9-e45a-4776f47f0667",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bpemb_en = BPEmb(lang=\"en\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 400869/400869 [00:00<00:00, 903239.32B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3784656/3784656 [00:00<00:00, 5322966.99B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnWzITC8qCvw",
        "outputId": "a78dc0c5-d84b-4e15-971a-8bd76676fac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bpemb_en.vectors.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaV2IRzjEClD"
      },
      "source": [
        "Let's create a function to load the corpus data (the books):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVl_hJ8yNktz"
      },
      "source": [
        "def get_file(filename = \"frankenstein.txt\"):\n",
        "  path = tf.keras.utils.get_file(\n",
        "      filename, origin=f\"https://raw.githubusercontent.com/liadmagen/NLP-Course/master/dataset/{filename}\"\n",
        "  )\n",
        "  with open(path, encoding=\"utf-8\") as f:\n",
        "      text = f.read() \n",
        "  text = text.replace(\"\\n\", \" \")        # Remove line-breaks & newlines\n",
        "  print(\"Corpus length:\", len(text))\n",
        "  return text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2iZ2Ol3EbU9"
      },
      "source": [
        "# RNN Model\n",
        "And this is the model itself. This is a very raw structure of it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a768GRY4TXVH"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ninp, noutp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "          ninp =  LSTM input size \n",
        "          noutp = size of the output (number of classes)\n",
        "          nhid = number of neurons in the hidden layer\n",
        "          nlayers = number of hidden layer\n",
        "          dropout = dropout rate\n",
        "          tie_weights = whether to use tie_weights (see note)\n",
        "        \"\"\"\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.noutp = noutp\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder = nn.Embedding.from_pretrained(tensor(bpemb_en.vectors))\n",
        "        # Freeze the embedding - don't let them be trained\n",
        "        self.encoder.weight.requires_grad = False\n",
        "        \n",
        "        # self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity='relu', dropout=dropout)\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "\n",
        "        self.decoder = nn.Linear(nhid, noutp)\n",
        "\n",
        "        # Optionally tie weights as in:\n",
        "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "        # https://arxiv.org/abs/1608.05859\n",
        "        # and\n",
        "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
        "        # https://arxiv.org/abs/1611.01462\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to ninp (embedding size)')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.weight)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.noutp)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, batch_size, self.nhid))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsVkK0WRE4us"
      },
      "source": [
        "A helper class to convert the tokens into batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cc83Hwah0Hc"
      },
      "source": [
        "def batchify(data, batch_size):\n",
        "    # Work out how cleanly we can divide the dataset into batch_size parts.\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Evenly divide the data across the batch_size batches.\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    return data.to(device)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFFTghmZE8JO"
      },
      "source": [
        "Let's load the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJq1xx_Lj813",
        "outputId": "b4744847-00c2-4ef2-f46e-91941046cbad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_corpus = get_file('dracula.txt')\n",
        "val_corpus = get_file('frankenstein.txt')\n",
        "\n",
        "print(train_corpus[:300])\n",
        "print(val_corpus[:300])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/liadmagen/NLP-Course/master/dataset/dracula.txt\n",
            "860160/857524 [==============================] - 0s 0us/step\n",
            "Corpus length: 842159\n",
            "Downloading data from https://raw.githubusercontent.com/liadmagen/NLP-Course/master/dataset/frankenstein.txt\n",
            "434176/430265 [==============================] - 0s 0us/step\n",
            "Corpus length: 420726\n",
            "Dracula, by Bram Stoker  CHAPTER I  JONATHAN HARKER'S JOURNAL  (_Kept in shorthand._)   _3 May. Bistritz._--Left Munich at 8:35 P. M., on 1st May, arriving at Vienna early next morning; should have arrived at 6:46, but train was an hour late. Buda-Pesth seems a wonderful place, from the glimpse whic\n",
            "Frankenstein, or, the Modern Prometheus by Mary Wollstonecraft (Godwin) Shelley  Letter 1  _To Mrs. Saville, England._   St. Petersburgh, Dec. 11th, 17—.   You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rA2YyF8E9-5"
      },
      "source": [
        "# Semantic representation + word-parts\n",
        "\n",
        "And convert it into vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC4A4ygqiYDd"
      },
      "source": [
        "train_encoded_text = bpemb_en.encode(train_corpus)\n",
        "train_encoded_ids = bpemb_en.encode_ids(train_corpus)\n",
        "\n",
        "val_encoded_text = bpemb_en.encode(val_corpus)\n",
        "val_encoded_ids = bpemb_en.encode_ids(val_corpus)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfs4y-jI7de3",
        "outputId": "ec3c5ba9-a416-4132-c994-87dce8e9aaeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "val_encoded_ids[:20]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2285,\n",
              " 19,\n",
              " 3521,\n",
              " 9934,\n",
              " 127,\n",
              " 9934,\n",
              " 7,\n",
              " 1463,\n",
              " 1073,\n",
              " 8108,\n",
              " 70,\n",
              " 101,\n",
              " 2195,\n",
              " 15,\n",
              " 484,\n",
              " 3820,\n",
              " 1568,\n",
              " 64,\n",
              " 8323,\n",
              " 3084]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_KagnLz7s_1",
        "outputId": "2b6a3b54-ff0d-4a34-c036-cd25908e6a92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_encoded_ids[:10]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1187, 9924, 2206, 9934, 101, 473, 56, 66, 7468, 5468]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXmw96xvFEzL"
      },
      "source": [
        "Let's check the result of encoded_text (we'll get to encoded_ids in a moment).\n",
        "\n",
        "Notice that every word is now broken to pieces. \n",
        "\n",
        "A **'_'** mark in the beginning of a token, represents a beginning of a new word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oki7xTl5FDyT",
        "outputId": "93ff53d5-2bdf-4f2e-f4a1-4075f37e845a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_encoded_text[:50]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁dra',\n",
              " 'c',\n",
              " 'ula',\n",
              " ',',\n",
              " '▁by',\n",
              " '▁br',\n",
              " 'am',\n",
              " '▁st',\n",
              " 'oker',\n",
              " '▁chapter',\n",
              " '▁i',\n",
              " '▁jonathan',\n",
              " '▁har',\n",
              " 'ker',\n",
              " \"'\",\n",
              " 's',\n",
              " '▁journal',\n",
              " '▁(',\n",
              " '_',\n",
              " 'ke',\n",
              " 'pt',\n",
              " '▁in',\n",
              " '▁sh',\n",
              " 'or',\n",
              " 'th',\n",
              " 'and',\n",
              " '.',\n",
              " '_',\n",
              " ')',\n",
              " '▁',\n",
              " '_',\n",
              " '0',\n",
              " '▁may',\n",
              " '.',\n",
              " '▁b',\n",
              " 'ist',\n",
              " 'rit',\n",
              " 'z',\n",
              " '.',\n",
              " '_',\n",
              " '-',\n",
              " '-',\n",
              " 'left',\n",
              " '▁mun',\n",
              " 'ich',\n",
              " '▁at',\n",
              " '▁0:00',\n",
              " '▁p',\n",
              " '.',\n",
              " '▁m']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkpMGwmYFR8E"
      },
      "source": [
        "This method is called word-parts. \n",
        "\n",
        "Instead of converting a whole word (word2vec, gloVe), or a character (FastText), this method converts slices of text, a combination of characters, together.\n",
        "\n",
        "It does so by finding the most common combinations, most frequent combinations, of characters in a very big corpus. \n",
        "\n",
        "The result is having a vocabulary which is WAY smaller than all-the-words (how big would that be?) bug bigger than all the characters:\n",
        "\n",
        "**character-based << word-piece based << word-based**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21-E5O3tF9HK"
      },
      "source": [
        "# Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGD8BiR7oT11"
      },
      "source": [
        "batch_size = 32\n",
        "eval_batch_size = 32\n",
        "\n",
        "vocab_size = bpemb_en.vocab_size\n",
        "embsize = bpemb_en.vectors.shape[1]\n",
        "nhidden = 256\n",
        "nlayers = 2"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAdLH-8YuILA"
      },
      "source": [
        "model = RNNModel(embsize, vocab_size, nhidden, nlayers).to(device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2g7-0EkqYZh"
      },
      "source": [
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ9zbXUwGBXd"
      },
      "source": [
        "# Division to train/validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyj2AlPwuLui"
      },
      "source": [
        "train_enc_ids = torch.tensor(train_encoded_ids).type(torch.int64)\n",
        "train_data = batchify(train_enc_ids, batch_size)\n",
        "\n",
        "val_enc_ids = torch.tensor(val_encoded_ids).type(torch.int64)\n",
        "val_data = batchify(val_enc_ids, batch_size)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaDDZuY7qyRH"
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj9TGJMkypM9"
      },
      "source": [
        "def get_batch(source, i):\n",
        "    seq_len = min(batch_size, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdvF1m90GGtJ"
      },
      "source": [
        "# Training function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG0lUe2SyF5c"
      },
      "source": [
        "Note: In 'real-ilfe' we're using helping frameworks such as [ignite](https://pytorch.org/ignite/) or [lightning](https://www.pytorchlightning.ai/). \n",
        "\n",
        "We bring it in this version here, for learning purposes only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLQmqGbuq39-"
      },
      "source": [
        "def train_epoch(train_data, optimizer, lr_scheduler, log_interval = 100):\n",
        "    # Turn on training mode - which enables dropout.\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.\n",
        "\n",
        "    start_time = time.time()\n",
        "    # ntokens = len(train_data)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, batch_size)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        \n",
        "        output, hidden = model(data, hidden)\n",
        "        \n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "\n",
        "        # update parameters (manually:)\n",
        "        # for p in model.parameters():\n",
        "        #   if p.grad is not None:\n",
        "        #     p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        # better with an optimizer:\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // batch_size, \n",
        "                lr_scheduler.get_last_lr()[0],\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REFFlXWO0MbI"
      },
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, batch_size):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvhCcxsTGPk0"
      },
      "source": [
        "# Training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSd0s9jUuA3O",
        "outputId": "7ae726a8-f55e-4d08-b2d8-b1511f2ca2ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Loop over epochs.\n",
        "lr = 5\n",
        "best_val_loss = None\n",
        "epochs = 20\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n",
        "                                                   max_lr=lr,\n",
        "                                                   epochs=epochs,\n",
        "                                                   steps_per_epoch=10)\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_epoch(train_data, optimizer, lr_scheduler)\n",
        "    \n",
        "    val_loss = evaluate(val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                        val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    # else:\n",
        "        # At this point, the learning rate is annealing if no improvement \n",
        "        # has been seen in the validation dataset. But we use pyTorch bult-in\n",
        "        # lr_scheduler for this.\n",
        "        # lr /= 2.0"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   100/  217 batches | lr 0.20 | ms/batch 12.26 | loss  6.39 | ppl   596.56\n",
            "| epoch   1 |   200/  217 batches | lr 0.20 | ms/batch 11.28 | loss  6.29 | ppl   538.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  2.91s | valid loss  6.87 | valid ppl   965.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   100/  217 batches | lr 0.20 | ms/batch 11.43 | loss  6.36 | ppl   578.53\n",
            "| epoch   2 |   200/  217 batches | lr 0.20 | ms/batch 11.33 | loss  6.26 | ppl   525.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  2.84s | valid loss  6.84 | valid ppl   935.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   100/  217 batches | lr 0.20 | ms/batch 11.46 | loss  6.33 | ppl   563.31\n",
            "| epoch   3 |   200/  217 batches | lr 0.20 | ms/batch 11.31 | loss  6.23 | ppl   509.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  2.84s | valid loss  6.84 | valid ppl   937.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   100/  217 batches | lr 0.20 | ms/batch 11.42 | loss  6.30 | ppl   544.61\n",
            "| epoch   4 |   200/  217 batches | lr 0.20 | ms/batch 11.29 | loss  6.19 | ppl   489.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time:  2.84s | valid loss  6.82 | valid ppl   913.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   100/  217 batches | lr 0.20 | ms/batch 11.45 | loss  6.25 | ppl   515.96\n",
            "| epoch   5 |   200/  217 batches | lr 0.20 | ms/batch 11.33 | loss  6.14 | ppl   462.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time:  2.84s | valid loss  6.77 | valid ppl   872.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   100/  217 batches | lr 0.20 | ms/batch 11.48 | loss  6.18 | ppl   484.04\n",
            "| epoch   6 |   200/  217 batches | lr 0.20 | ms/batch 11.38 | loss  6.08 | ppl   436.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time:  2.85s | valid loss  6.75 | valid ppl   853.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   100/  217 batches | lr 0.20 | ms/batch 11.45 | loss  6.13 | ppl   457.18\n",
            "| epoch   7 |   200/  217 batches | lr 0.20 | ms/batch 11.37 | loss  6.02 | ppl   413.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time:  2.85s | valid loss  6.73 | valid ppl   836.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   100/  217 batches | lr 0.20 | ms/batch 11.52 | loss  6.07 | ppl   431.45\n",
            "| epoch   8 |   200/  217 batches | lr 0.20 | ms/batch 11.39 | loss  5.97 | ppl   391.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time:  2.85s | valid loss  6.66 | valid ppl   782.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   100/  217 batches | lr 0.20 | ms/batch 11.53 | loss  6.02 | ppl   409.68\n",
            "| epoch   9 |   200/  217 batches | lr 0.20 | ms/batch 11.45 | loss  5.92 | ppl   374.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time:  2.86s | valid loss  6.65 | valid ppl   770.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   100/  217 batches | lr 0.20 | ms/batch 11.50 | loss  5.97 | ppl   389.95\n",
            "| epoch  10 |   200/  217 batches | lr 0.20 | ms/batch 11.40 | loss  5.88 | ppl   356.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time:  2.85s | valid loss  6.62 | valid ppl   748.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   100/  217 batches | lr 0.20 | ms/batch 11.54 | loss  5.92 | ppl   373.95\n",
            "| epoch  11 |   200/  217 batches | lr 0.20 | ms/batch 11.35 | loss  5.83 | ppl   341.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time:  2.85s | valid loss  6.63 | valid ppl   753.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   100/  217 batches | lr 0.20 | ms/batch 11.51 | loss  5.88 | ppl   359.47\n",
            "| epoch  12 |   200/  217 batches | lr 0.20 | ms/batch 11.35 | loss  5.79 | ppl   327.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time:  2.85s | valid loss  6.58 | valid ppl   719.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   100/  217 batches | lr 0.20 | ms/batch 11.56 | loss  5.85 | ppl   347.47\n",
            "| epoch  13 |   200/  217 batches | lr 0.20 | ms/batch 11.40 | loss  5.76 | ppl   316.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time:  2.86s | valid loss  6.56 | valid ppl   704.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   100/  217 batches | lr 0.20 | ms/batch 11.55 | loss  5.82 | ppl   336.31\n",
            "| epoch  14 |   200/  217 batches | lr 0.20 | ms/batch 11.38 | loss  5.73 | ppl   307.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time:  2.85s | valid loss  6.55 | valid ppl   695.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   100/  217 batches | lr 0.20 | ms/batch 11.54 | loss  5.79 | ppl   326.43\n",
            "| epoch  15 |   200/  217 batches | lr 0.20 | ms/batch 11.37 | loss  5.70 | ppl   297.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time:  2.85s | valid loss  6.53 | valid ppl   687.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   100/  217 batches | lr 0.20 | ms/batch 11.55 | loss  5.76 | ppl   316.79\n",
            "| epoch  16 |   200/  217 batches | lr 0.20 | ms/batch 11.47 | loss  5.67 | ppl   289.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time:  2.86s | valid loss  6.53 | valid ppl   682.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   100/  217 batches | lr 0.20 | ms/batch 11.58 | loss  5.73 | ppl   309.18\n",
            "| epoch  17 |   200/  217 batches | lr 0.20 | ms/batch 11.37 | loss  5.64 | ppl   282.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time:  2.86s | valid loss  6.51 | valid ppl   674.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   100/  217 batches | lr 0.20 | ms/batch 11.63 | loss  5.71 | ppl   301.30\n",
            "| epoch  18 |   200/  217 batches | lr 0.20 | ms/batch 11.34 | loss  5.62 | ppl   276.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time:  2.86s | valid loss  6.50 | valid ppl   667.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   100/  217 batches | lr 0.20 | ms/batch 11.52 | loss  5.68 | ppl   292.36\n",
            "| epoch  19 |   200/  217 batches | lr 0.20 | ms/batch 11.41 | loss  5.59 | ppl   268.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time:  2.86s | valid loss  6.48 | valid ppl   653.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   100/  217 batches | lr 0.20 | ms/batch 11.57 | loss  5.66 | ppl   285.90\n",
            "| epoch  20 |   200/  217 batches | lr 0.20 | ms/batch 11.39 | loss  5.57 | ppl   262.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time:  2.87s | valid loss  6.49 | valid ppl   658.51\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq_PkPQOGVcB"
      },
      "source": [
        "# Text Generation example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "477FsXfZuDQ3",
        "outputId": "949ebe8e-14c6-4cdf-9d15-9f9c2cdd46d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.eval()\n",
        "\n",
        "log_interval = 100\n",
        "words_to_generate = 50\n",
        "temperature = 1. # higher temperature will increase diversity\n",
        "\n",
        "# generate random start\n",
        "input = torch.randint(10000, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "hidden = model.init_hidden(1)\n",
        "\n",
        "generated_word_ids = []\n",
        "\n",
        "with torch.no_grad():  # no tracking history\n",
        " for i in range(words_to_generate):\n",
        "    output, hidden = model(input, hidden)\n",
        "    word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "    input.fill_(word_idx)\n",
        "\n",
        "    generated_word_ids.append(word_idx.tolist())\n",
        "\n",
        "bpemb_en.decode_ids(generated_word_ids)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"exists of theum in a pen of his that. jonathan greatly isive, hur lainn, and ifat des, and he madenentlycript and ax, and itself he went figures to keep back that i could them from in '\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI9JwCE8GZcD"
      },
      "source": [
        "As discussed in class, the RNN/LSTM can be used to many various task:\n",
        "\n",
        "it can be used for sequence2sequence, where the sequence size is the same or different: \n",
        "* Translation\n",
        "* Tagging words as POS / SLR / NER\n",
        "* Encoding a document as a vector for classification\n",
        "\n",
        "etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LumNOdKFaKI"
      },
      "source": [
        "# Your Turn:\n",
        "\n",
        "Let's practice LSTM.\n",
        "\n",
        "Train a sentiment anlalysis on the Stanford Sentiment Treebank (SST).\n",
        "\n",
        "You will need to:\n",
        "* Change the network output to produce a score, instead of class (Think: which loss function would you use for that matter?)\n",
        "* Use bpEmb to vectorize the sentences\n",
        "* divide your training set to Train + Validation + Hold-out sets - use split_df\n",
        "* Change the training + validation loops to compute the loss over a whole sentence, and not for every word. \n",
        "* Add a test set (very similar to the validation set) to check your network score\n",
        "\n",
        "Q: What is the network output after the whole sentence is processed? \n",
        "Hint: https://www.aclweb.org/anthology/P18-1198.pdf\n",
        "\n",
        "Q: Here we trined a language model over a simple book. Which datasource(s) would you choosoe to train a language model for this dataset?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgOuj_7qFaAr"
      },
      "source": [
        "## Setup & DS download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Q25uV2XDSEQ",
        "outputId": "f079917b-9858-4748-efa5-130e75115b42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget 'https://raw.githubusercontent.com/liadmagen/NLP-Course/master/sst/datasetSentences.txt'\n",
        "!wget 'https://raw.githubusercontent.com/liadmagen/NLP-Course/master/sst/sentiment_labels.txt'\n",
        "!wget 'https://raw.githubusercontent.com/liadmagen/NLP-Course/master/sst/datasetSplit.txt'\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-10 16:54:21--  https://raw.githubusercontent.com/liadmagen/NLP-Course/master/sst/datasetSentences.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1290263 (1.2M) [text/plain]\n",
            "Saving to: ‘datasetSentences.txt.1’\n",
            "\n",
            "\rdatasetSentences.tx   0%[                    ]       0  --.-KB/s               \rdatasetSentences.tx 100%[===================>]   1.23M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-11-10 16:54:21 (31.3 MB/s) - ‘datasetSentences.txt.1’ saved [1290263/1290263]\n",
            "\n",
            "--2020-11-10 16:54:21--  https://raw.githubusercontent.com/liadmagen/NLP-Course/master/sst/sentiment_labels.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3263577 (3.1M) [text/plain]\n",
            "Saving to: ‘sentiment_labels.txt’\n",
            "\n",
            "sentiment_labels.tx 100%[===================>]   3.11M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-11-10 16:54:21 (56.6 MB/s) - ‘sentiment_labels.txt’ saved [3263577/3263577]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edohps5s6l25"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVHBGlqtDP2X"
      },
      "source": [
        "data_df = pd.read_csv('datasetSentences.txt', sep='\\t', index_col=0)\n",
        "label_df = pd.read_csv('sentiment_labels.txt', sep='|', index_col=0)\n",
        "split_df = pd.read_csv('datasetSplit.txt', sep='|', index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7wkneSoFfNl"
      },
      "source": [
        "## Look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i971rg8pEloI",
        "outputId": "4c4c5059-bfa7-42a3-bd58-498890e98509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        " df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentence_index</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>The film provides some great insight into the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Offers that rare combination of entertainment ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Perhaps no picture ever made has more literall...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Steers turns in a snappy screenplay that curls...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>But he somehow pulls it off .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         sentence\n",
              "sentence_index                                                   \n",
              "1               The Rock is destined to be the 21st Century 's...\n",
              "2               The gorgeously elaborate continuation of `` Th...\n",
              "3                                  Effective but too-tepid biopic\n",
              "4               If you sometimes like to go to the movies to h...\n",
              "5               Emerges as something rare , an issue movie tha...\n",
              "6               The film provides some great insight into the ...\n",
              "7               Offers that rare combination of entertainment ...\n",
              "8               Perhaps no picture ever made has more literall...\n",
              "9               Steers turns in a snappy screenplay that curls...\n",
              "10                                  But he somehow pulls it off ."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4H4NrgdFSdw",
        "outputId": "703aebdc-6eb5-43d5-ad91-d9ef21804ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "label_df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment values</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>phrase ids</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.44444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.50000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.42708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.37500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.41667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.54167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.33333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.45833</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            sentiment values\n",
              "phrase ids                  \n",
              "0                    0.50000\n",
              "1                    0.50000\n",
              "2                    0.44444\n",
              "3                    0.50000\n",
              "4                    0.42708\n",
              "5                    0.37500\n",
              "6                    0.41667\n",
              "7                    0.54167\n",
              "8                    0.33333\n",
              "9                    0.45833"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMnFwbJgFUXr"
      },
      "source": [
        "split_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}