{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "asigmo_ex2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahnazshamissa/Python/blob/main/asigmo_ex2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-3Byol5O_U5"
      },
      "source": [
        "# Lab exercise 2\n",
        "\n",
        "\n",
        "Per Group:\n",
        "1.   Create a teams-meeting where one person opens the colab notebook and shares the screen (preferably a different person than yesterday)\n",
        "2.   Change the network as described below. Different to yesterday you need to use Google and read the Tensorflow documention.\n",
        "3.   Train the network, then plot the training history and evaluate it one the test set\n",
        "4.   Replace the code that plots the training history by code that computes the best validation accuracy achieved during the training\n",
        "5.   Post the best validation accuracy during and the test accuracy in the general Deep Learning teams channel\n",
        "6.   Think about why your network performed better or worse compared to the ~73% accuracy of the baseline network\n",
        "7.   In each group channel answer the questions below\n",
        "\n",
        "\n",
        "- **Group A:** Add random zoom data augmentation (before the RandomCrop layer) and try it out with a height_factor of 0.1 and 0.2.\n",
        "- **Group C:** Add L2 regularization with a factor of 0.0001 to the kernel of the convolutional layers. Try it out, then increase the learning rate to 0.001 and try it again\n",
        "- **Group D:** Add [random flip data augmentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/RandomFlip) before the rescaling layer. Try it out once with only horzontal flips and once with both horizontal and vertical flips \n",
        "- **Group E:** Add an spacial 2D dropout layer after the first MaxPool2D layer. Try is out with a dropout rate of 0.1 and 0.2\n",
        "\n",
        "\n",
        "**Questions**\n",
        "1.  How many training samples are fed through the network in each epochs? \n",
        "2.  How many weight updates (= training steps) are applied in our case in each epoch?\n",
        "3.  How many training steps are applied in our case over the whole training process?\n",
        "4.  Read the documentation of [```tf.keras.optimizers.schedules.PiecewiseConstantDecay```](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/PiecewiseConstantDecay). What does this module do?\n",
        "5.  Which values for the argument ```boundaries``` and ```values``` do we need to set if we can a learning rate of 0.01 for the first 50 epochs, 0.001 for the next next 50 epochs and 0.0001 for the last 50 epochs?\n",
        "\n",
        "**Info**: If your Colab notebook is training in the background (another tab), open it once in a while to avoid Colab from disconnecting the session!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB-q8XI3NPWq"
      },
      "source": [
        "## Setup notebook environment\n",
        "\n",
        "Make sure the Colab environment has a GPU enabled *Edit->Notebook Settings->Hardware accelerator->Choose GPU*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aG_RlnMI7Fj"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc8c_aozNWpg"
      },
      "source": [
        "Download the dataset first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGa7l7SjNZFe"
      },
      "source": [
        "! wget https://pub.ist.ac.at/~mlechner/datasets/f8d.tar.gz\n",
        "! tar -xf f8d.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkhCfNnTOdTJ"
      },
      "source": [
        "Import python libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBF6BqbgNbII"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlKQkw7ZOhBQ"
      },
      "source": [
        "## Step 1: Define datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZt14huyNguI"
      },
      "source": [
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  \"f8d/test\", shuffle=False, batch_size=128, image_size=(256, 256),\n",
        ")\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  \"f8d/training\",\n",
        "  shuffle=True,\n",
        "  batch_size=64,\n",
        "  image_size=(256, 256),\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=1020202,\n",
        ")\n",
        "valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  \"f8d/training\",\n",
        "  shuffle=True,\n",
        "  batch_size=128,\n",
        "  image_size=(256, 256),\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=1020202,\n",
        ")\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "valid_dataset = valid_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HtXmk8mOrh4"
      },
      "source": [
        "## Step 2 and 3: Define model and objective"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onpKWbSNNjCg"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Input(shape=(256,256,3)),\n",
        "  tf.keras.layers.experimental.preprocessing.RandomCrop(224, 224),\n",
        "  tf.keras.layers.experimental.preprocessing.Rescaling(1.0 / 255),\n",
        "  tf.keras.layers.Conv2D(64, 7, strides=2, activation=\"relu\", padding=\"same\"),\n",
        "  tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"),\n",
        "  tf.keras.layers.MaxPool2D(),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Conv2D(128,(4, 4), activation=\"relu\", padding=\"same\"),\n",
        "  tf.keras.layers.MaxPool2D(),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Conv2D(256,(3, 3), activation=\"relu\", padding=\"same\"),\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(8,activation=\"softmax\"),\n",
        "])\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.0005),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXRe8j9zOvbw"
      },
      "source": [
        "## Step 4: Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Aes5WmpNlTN"
      },
      "source": [
        "training_log = model.fit(\n",
        "  train_dataset, epochs=150, validation_data=valid_dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAoj-WjROyUI"
      },
      "source": [
        "**Task:** Instead of plotting the training history, compute the best validation accuracy achieved during the entire training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIt6mEA-NnIY"
      },
      "source": [
        "fig,axes = plt.subplots(2,1,figsize=(10,10))\n",
        "axes[0].plot(training_log.history[\"loss\"],label=\"Training loss\")\n",
        "axes[0].plot(training_log.history[\"val_loss\"],label=\"Validation loss\")\n",
        "axes[0].legend(loc=\"upper right\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[1].plot(training_log.history[\"sparse_categorical_accuracy\"],label=\"Training accuracy\")\n",
        "axes[1].plot(training_log.history[\"val_sparse_categorical_accuracy\"],label=\"Validation accuracy\")\n",
        "axes[1].legend(loc=\"upper right\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eECMpKvLO4LJ"
      },
      "source": [
        "## Step 5: Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP1f7owVNohQ"
      },
      "source": [
        "model.evaluate(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}