{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "asigmo_ex4_fatemeh_dracula.txt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahnazshamissa/Python/blob/main/asigmo_ex4_fatemeh_dracula_txt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLvl9qZpRR1A"
      },
      "source": [
        "# Exercise 4\n",
        "\n",
        "\n",
        "Per group create a teams-meeting where one person opens the colab notebook and shares the screen (preferably a different person than yesterday)\n",
        "\n",
        "1.   Change the data source from \"frankenstein.txt\" to \"dracula.txt\", \n",
        "2.   Train the network for 10 epochs and test it on the 3 example sentences\n",
        "3.   Write down 3 more example sentences and test the model on these as well\n",
        "4.   Train the LSTM for 50 more epochs and run the completion again. What do you observe?\n",
        "4.   Add another LSTM layer to our RNN model\n",
        "5.   Train the 2-layer LSTM for 10 epochs and see how the sentence completion has changed\n",
        "6.   Change the \"dracula.txt\" to \"trump.txt\", which contains Tweets from Donald Trump from 2019 and 2020.\n",
        "7.   Train a single-layer LSTM on the Tweet's from Trump. If you run out of memory while preprocessing the data, change ```step = 3``` to ```step = 10```.\n",
        "8.   Run the completion on the six sentences from before again. What do you observe?\n",
        "\n",
        "\n",
        "\n",
        "**Question**\n",
        "The models make very few spelling errors and few grammar error, but the content is mostly nonsense. Why do you think are the error types so different?\n",
        "\n",
        "We will meet in the general channel at 12:10!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A4xscds3KtQ",
        "outputId": "c9669a06-ad18-431b-f564-5ca814b6b866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Oct 30 11:00:02 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.32.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    31W /  70W |    645MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTLXcbOl3n2E"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He7DevTX5gcs"
      },
      "source": [
        "# Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhAlKCjm5f0b",
        "outputId": "6ff96982-5977-43b3-f558-7828d153b003",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "filename = \"dracula.txt\"  \n",
        "path = tf.keras.utils.get_file(\n",
        "    filename, origin=f\"https://pub.ist.ac.at/~mlechner/datasets/{filename}\"\n",
        ")\n",
        "with open(path, encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()           # Make lowercase \n",
        "text = text.replace(\"\\n\", \" \")        # Remove line-breaks & newlines\n",
        "print(\"Corpus length:\", len(text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://pub.ist.ac.at/~mlechner/datasets/dracula.txt\n",
            "860160/857524 [==============================] - 1s 1us/step\n",
            "Corpus length: 842159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtFFQXZB5pfj",
        "outputId": "f95c1eec-6498-4e67-bb57-5f88385c3c39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print the first 100 characters of the file we downloaded\n",
        "print(\"First 100 characters: \",text[:100])\n",
        "print(\"Last 100 characters : \",text[-100:])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 100 characters:  dracula, by bram stoker  chapter i  jonathan harker's journal  (_kept in shorthand._)   _3 may. bist\n",
            "Last 100 characters :  will understand how some men so loved her, that they did dare much for her sake.\"  jonathan harker. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ6Kxxoo6Pg1"
      },
      "source": [
        "# Define input features and output labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFTI5-543yKi",
        "outputId": "5d3f50d6-2cc7-4faf-af9c-e7706beacf40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(\"Total chars:\", len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "maxlen = 50\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i : i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print(\"Number of sequences:\", len(sentences))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total chars: 64\n",
            "Number of sequences: 280703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z75uMG-35Ihz",
        "outputId": "579477ab-3ba7-4213-d855-19f13bd54727",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"First sentence:\")\n",
        "print(sentences[0])\n",
        "print(\"First label: \")\n",
        "print(next_chars[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First sentence:\n",
            "dracula, by bram stoker  chapter i  jonathan harke\n",
            "First label: \n",
            "r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eIkuFt65aQF"
      },
      "source": [
        "# Transforming text into numerical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJP69sDV5VGj"
      },
      "source": [
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros(len(sentences), dtype=np.int32)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i] = char_indices[next_chars[i]]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KbhiVJ140xE",
        "outputId": "d3bf6783-310f-4929-8d99-6fbc23074b8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# First training samples\n",
        "print(x[0])\n",
        "print(\"label: \")\n",
        "print(y[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " ...\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]\n",
            " [False False False ... False False False]]\n",
            "label: \n",
            "42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwfhq3pnEBit"
      },
      "source": [
        "# Splitting training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW4OyUE4DZS9",
        "outputId": "7d1c85a2-d073-4c18-84d7-04af9c0dbda9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We can define our training and validation dataset as simple numpy arrays\n",
        "split_index = int(0.2*x.shape[0])\n",
        "valid_x, valid_y = x[-split_index:],y[-split_index:]\n",
        "train_x, train_y = x[:-split_index],y[:-split_index]\n",
        "print(\"valid_x.shape\",valid_x.shape)\n",
        "print(\"valid_y.shape\",valid_y.shape)\n",
        "print(\"train_x.shape\",train_x.shape)\n",
        "print(\"train_y.shape\",train_y.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "valid_x.shape (56140, 50, 64)\n",
            "valid_y.shape (56140,)\n",
            "train_x.shape (224563, 50, 64)\n",
            "train_y.shape (224563,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCurnl9YOAM1"
      },
      "source": [
        "# Defining our Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se-1ndKD4E4i",
        "outputId": "e41e324d-4437-49e2-8189-421815de09b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(maxlen, len(chars))),\n",
        "        tf.keras.layers.LSTM(128),\n",
        "        tf.keras.layers.Dense(len(chars), activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,metrics=[\"sparse_categorical_accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                8256      \n",
            "=================================================================\n",
            "Total params: 107,072\n",
            "Trainable params: 107,072\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ0PMEJlRDyd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1NLlj4z4bkK",
        "outputId": "89742f6b-e7e0-495a-829b-a5f4e39932f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_log = model.fit(train_x, train_y, batch_size=64, epochs=50, validation_data=(valid_x,valid_y))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "3509/3509 [==============================] - 20s 6ms/step - loss: 1.8259 - sparse_categorical_accuracy: 0.4656 - val_loss: 1.6234 - val_sparse_categorical_accuracy: 0.5201\n",
            "Epoch 2/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 1.5550 - sparse_categorical_accuracy: 0.5376 - val_loss: 1.5580 - val_sparse_categorical_accuracy: 0.5358\n",
            "Epoch 3/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 1.4926 - sparse_categorical_accuracy: 0.5547 - val_loss: 1.5252 - val_sparse_categorical_accuracy: 0.5458\n",
            "Epoch 4/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 1.4593 - sparse_categorical_accuracy: 0.5634 - val_loss: 1.5210 - val_sparse_categorical_accuracy: 0.5506\n",
            "Epoch 5/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 1.4402 - sparse_categorical_accuracy: 0.5680 - val_loss: 1.5023 - val_sparse_categorical_accuracy: 0.5571\n",
            "Epoch 6/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 1.4238 - sparse_categorical_accuracy: 0.5719 - val_loss: 1.5030 - val_sparse_categorical_accuracy: 0.5557\n",
            "Epoch 7/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 1.4114 - sparse_categorical_accuracy: 0.5761 - val_loss: 1.5047 - val_sparse_categorical_accuracy: 0.5571\n",
            "Epoch 8/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 1.3999 - sparse_categorical_accuracy: 0.5792 - val_loss: 1.5005 - val_sparse_categorical_accuracy: 0.5602\n",
            "Epoch 9/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 1.3914 - sparse_categorical_accuracy: 0.5804 - val_loss: 1.4895 - val_sparse_categorical_accuracy: 0.5594\n",
            "Epoch 10/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 1.3879 - sparse_categorical_accuracy: 0.5810 - val_loss: 1.4994 - val_sparse_categorical_accuracy: 0.5612\n",
            "Epoch 11/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 1.3837 - sparse_categorical_accuracy: 0.5827 - val_loss: 1.4962 - val_sparse_categorical_accuracy: 0.5597\n",
            "Epoch 12/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 1.3837 - sparse_categorical_accuracy: 0.5833 - val_loss: 1.5030 - val_sparse_categorical_accuracy: 0.5593\n",
            "Epoch 13/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 1.3813 - sparse_categorical_accuracy: 0.5837 - val_loss: 1.5185 - val_sparse_categorical_accuracy: 0.5587\n",
            "Epoch 14/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 1.5029 - sparse_categorical_accuracy: 0.5686 - val_loss: 1.9412 - val_sparse_categorical_accuracy: 0.4980\n",
            "Epoch 15/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.2072 - sparse_categorical_accuracy: 0.4442 - val_loss: 2.4261 - val_sparse_categorical_accuracy: 0.3851\n",
            "Epoch 16/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 3.0044 - sparse_categorical_accuracy: 0.2880 - val_loss: 2.8789 - val_sparse_categorical_accuracy: 0.2759\n",
            "Epoch 17/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.7129 - sparse_categorical_accuracy: 0.2799 - val_loss: 2.7072 - val_sparse_categorical_accuracy: 0.2655\n",
            "Epoch 18/50\n",
            "3509/3509 [==============================] - 20s 6ms/step - loss: 2.7314 - sparse_categorical_accuracy: 0.2597 - val_loss: 2.7752 - val_sparse_categorical_accuracy: 0.2555\n",
            "Epoch 19/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.7773 - sparse_categorical_accuracy: 0.2460 - val_loss: 2.7830 - val_sparse_categorical_accuracy: 0.2454\n",
            "Epoch 20/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.7986 - sparse_categorical_accuracy: 0.2363 - val_loss: 2.7956 - val_sparse_categorical_accuracy: 0.2352\n",
            "Epoch 21/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.8099 - sparse_categorical_accuracy: 0.2305 - val_loss: 2.8264 - val_sparse_categorical_accuracy: 0.2251\n",
            "Epoch 22/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.8343 - sparse_categorical_accuracy: 0.2230 - val_loss: 2.8141 - val_sparse_categorical_accuracy: 0.2274\n",
            "Epoch 23/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.8575 - sparse_categorical_accuracy: 0.2171 - val_loss: 2.8782 - val_sparse_categorical_accuracy: 0.2152\n",
            "Epoch 24/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.8986 - sparse_categorical_accuracy: 0.2110 - val_loss: 2.8897 - val_sparse_categorical_accuracy: 0.2130\n",
            "Epoch 25/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9126 - sparse_categorical_accuracy: 0.2077 - val_loss: 2.8976 - val_sparse_categorical_accuracy: 0.2144\n",
            "Epoch 26/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9139 - sparse_categorical_accuracy: 0.2092 - val_loss: 2.9124 - val_sparse_categorical_accuracy: 0.2126\n",
            "Epoch 27/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9356 - sparse_categorical_accuracy: 0.2094 - val_loss: 2.9130 - val_sparse_categorical_accuracy: 0.2129\n",
            "Epoch 28/50\n",
            "3509/3509 [==============================] - 19s 6ms/step - loss: 2.9310 - sparse_categorical_accuracy: 0.2076 - val_loss: 2.9313 - val_sparse_categorical_accuracy: 0.2089\n",
            "Epoch 29/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9317 - sparse_categorical_accuracy: 0.2082 - val_loss: 2.9210 - val_sparse_categorical_accuracy: 0.2114\n",
            "Epoch 30/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9368 - sparse_categorical_accuracy: 0.2094 - val_loss: 2.9240 - val_sparse_categorical_accuracy: 0.2170\n",
            "Epoch 31/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9512 - sparse_categorical_accuracy: 0.2080 - val_loss: 2.9428 - val_sparse_categorical_accuracy: 0.2142\n",
            "Epoch 32/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9529 - sparse_categorical_accuracy: 0.2073 - val_loss: 2.9720 - val_sparse_categorical_accuracy: 0.2104\n",
            "Epoch 33/50\n",
            "3509/3509 [==============================] - 19s 6ms/step - loss: 2.9613 - sparse_categorical_accuracy: 0.2078 - val_loss: 2.9207 - val_sparse_categorical_accuracy: 0.2133\n",
            "Epoch 34/50\n",
            "3509/3509 [==============================] - 20s 6ms/step - loss: 2.9591 - sparse_categorical_accuracy: 0.2075 - val_loss: 2.9650 - val_sparse_categorical_accuracy: 0.1980\n",
            "Epoch 35/50\n",
            "3509/3509 [==============================] - 19s 6ms/step - loss: 2.9661 - sparse_categorical_accuracy: 0.2047 - val_loss: 2.9675 - val_sparse_categorical_accuracy: 0.2078\n",
            "Epoch 36/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9902 - sparse_categorical_accuracy: 0.2035 - val_loss: 2.9735 - val_sparse_categorical_accuracy: 0.2088\n",
            "Epoch 37/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9935 - sparse_categorical_accuracy: 0.2022 - val_loss: 2.9938 - val_sparse_categorical_accuracy: 0.1952\n",
            "Epoch 38/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9933 - sparse_categorical_accuracy: 0.2008 - val_loss: 2.9655 - val_sparse_categorical_accuracy: 0.2015\n",
            "Epoch 39/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9876 - sparse_categorical_accuracy: 0.2004 - val_loss: 2.9927 - val_sparse_categorical_accuracy: 0.2047\n",
            "Epoch 40/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9925 - sparse_categorical_accuracy: 0.1990 - val_loss: 2.9847 - val_sparse_categorical_accuracy: 0.2011\n",
            "Epoch 41/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9807 - sparse_categorical_accuracy: 0.2024 - val_loss: 2.9815 - val_sparse_categorical_accuracy: 0.2003\n",
            "Epoch 42/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9878 - sparse_categorical_accuracy: 0.2009 - val_loss: 2.9830 - val_sparse_categorical_accuracy: 0.2009\n",
            "Epoch 43/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 2.9883 - sparse_categorical_accuracy: 0.2003 - val_loss: 2.9754 - val_sparse_categorical_accuracy: 0.2021\n",
            "Epoch 44/50\n",
            "3509/3509 [==============================] - 19s 6ms/step - loss: 2.9968 - sparse_categorical_accuracy: 0.1997 - val_loss: 2.9968 - val_sparse_categorical_accuracy: 0.1992\n",
            "Epoch 45/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 3.0015 - sparse_categorical_accuracy: 0.1995 - val_loss: 2.9880 - val_sparse_categorical_accuracy: 0.2034\n",
            "Epoch 46/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 3.0077 - sparse_categorical_accuracy: 0.1988 - val_loss: 2.9975 - val_sparse_categorical_accuracy: 0.1998\n",
            "Epoch 47/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 3.0161 - sparse_categorical_accuracy: 0.1971 - val_loss: 3.0089 - val_sparse_categorical_accuracy: 0.1996\n",
            "Epoch 48/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 3.0177 - sparse_categorical_accuracy: 0.1963 - val_loss: 3.0053 - val_sparse_categorical_accuracy: 0.2022\n",
            "Epoch 49/50\n",
            "3509/3509 [==============================] - 19s 5ms/step - loss: 3.0083 - sparse_categorical_accuracy: 0.2004 - val_loss: 2.9931 - val_sparse_categorical_accuracy: 0.2039\n",
            "Epoch 50/50\n",
            "3509/3509 [==============================] - 20s 6ms/step - loss: 3.0261 - sparse_categorical_accuracy: 0.1980 - val_loss: 2.9992 - val_sparse_categorical_accuracy: 0.2033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-F2scl-RFjz"
      },
      "source": [
        "# Let's try out our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGqx_9q-A6kb"
      },
      "source": [
        "def sample(preds, temperature=0.5):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "    \n",
        "def complete_sentence(sentence):\n",
        "  if len(sentence) < maxlen:\n",
        "    raise ValueError(f\"Sentence must have at least {maxlen} characters, but provided sentence had only {sentence}\")\n",
        "  if len(sentence) > maxlen:\n",
        "    sentence = sentence[-maxlen:]\n",
        "  generated = \"\"\n",
        "  print(f\"Continuing sentence '{sentence}' ...\")\n",
        "\n",
        "  for i in range(200):\n",
        "      x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "      for t, char in enumerate(sentence):\n",
        "          x_pred[0, t, char_indices[char]] = 1.0\n",
        "      preds = model.predict(x_pred, verbose=0)[0]\n",
        "      next_index = sample(preds)\n",
        "      next_char = indices_char[next_index]\n",
        "      sentence = sentence[1:] + next_char\n",
        "      generated += next_char\n",
        "\n",
        "  print(\"... \", generated)\n",
        "  print()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZAW4zpVBgzj",
        "outputId": "97d6eb41-5abf-4362-b925-5d969d54eeb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "complete_sentence(\"this is an example to show how the machine would continue\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Continuing sentence ' an example to show how the machine would continue' ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...   aarngh  k e w n renia cste  e  o m li tr in er sgheaoh s  h a o it i bs   h p   io h thg t lsnnt  th w oe a cot  a h e  ohe uur  wan st t at oh dtronat htr yo  wi  hehe  h a ihe ie s  a auan anr t o \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQBfHwCJUmh2"
      },
      "source": [
        "### TODO: Add 3 more sentences to complete. Think about what types of sentences would be interesting to test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtcRdawJPE9s"
      },
      "source": [
        "# Biased data -> biased model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij4obo3uPKOq",
        "outputId": "df5a20e1-9821-4170-fbbc-5f8050ce705d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "complete_sentence(\"    was the thiefs answer, to which the man replied \")\n",
        "complete_sentence(\"    was the thiefs answer, to which the woman replied \")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Continuing sentence '  was the thiefs answer, to which the man replied ' ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...  b  a  t.n w ot  ntoeh rah ad \"tre  h wh p  h  t te t  hhia,  bo m be \"ee  of i h te aan  rile aht tl lr ois o iatan t ne   nda  scov aa    f ht  atn  yuhth o a un t otn u  ks s b t oe  wt  hss  wh nth\n",
            "\n",
            "Continuing sentence 'was the thiefs answer, to which the woman replied ' ...\n",
            "...  a th or  no  w im i t o r tt aa aei  at o uoueee-  att t  ao  udo she vic aa tic  th to oe  tht he t m  th sha a h ohr t et   t fnle a a i e  ih ih- ald t aaa  t ; ain t  a sth t  an h   w tat  s e c \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}