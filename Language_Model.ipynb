{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language_Model",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahnazshamissa/Python/blob/main/Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHpU4n2ZAd2p"
      },
      "source": [
        "# Language Model Example\n",
        "\n",
        "In this notebook, you'll train a n-gram Language model yourself"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYs_Dhab9syd"
      },
      "source": [
        "This example is based on:\n",
        "\n",
        "https://nlpforhackers.io/language-models/\n",
        "\n",
        "For a (very!) detailed information about this topic, please refer to:\n",
        "https://web.stanford.edu/~jurafsky/slp3/3.pdf "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxk8CHMfAr1H"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piIx18HE_dlq",
        "outputId": "ee7e595f-ea21-49c6-d8a1-2ae1180d00e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjCnv0889m7r"
      },
      "source": [
        "import random\n",
        "from functools import reduce\n",
        "\n",
        "from operator import mul\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        " "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj4-vpdmAuLu"
      },
      "source": [
        "# Bag-of-words & Frequency calculation\n",
        "\n",
        "We start by constructing a Bag-of-words.\n",
        "\n",
        "Remind yourself that a Language Model is a calculation of the frequencies and the probabilities of words, based on their observed occurrences.\n",
        "\n",
        "We calculate the word frequencies, by simply counting the word occurrences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI732WCZ9wsv",
        "outputId": "4ae53499-727e-415f-f2ec-fc3fd84b17df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "counts = Counter(reuters.words())\n",
        "total_count = len(reuters.words())\n",
        "\n",
        "print(counts.most_common(n=20))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('.', 94687), (',', 72360), ('the', 58251), ('of', 35979), ('to', 34035), ('in', 26478), ('said', 25224), ('and', 25043), ('a', 23492), ('mln', 18037), ('vs', 14120), ('-', 13705), ('for', 12785), ('dlrs', 11730), (\"'\", 11272), ('The', 10968), ('000', 10277), ('1', 9977), ('s', 9298), ('pct', 9093)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnxBoLt6-Rsz",
        "outputId": "84ce5b0c-e7ee-4cbc-9c4b-c7e60800f13b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Compute the frequencies\n",
        "for word in counts:\n",
        "    counts[word] /= float(total_count)\n",
        "\n",
        "# The frequencies should add up to 1\n",
        "print(sum(counts.values()))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000000000006808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzx-cyghBJzA"
      },
      "source": [
        "Let's create a random text passage, and calculate what is the probability that such a sentence is valid.\n",
        "\n",
        "Before continuing, and executing the next cells: \n",
        "\n",
        "**please pause and guess, what do you expect that the probability of a random 5 words passage would be? How about a 100?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4FefXrB9_YN",
        "outputId": "0cf376d6-f5f4-4862-919d-408770589b94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " # Generate 100 words of language\n",
        "text = []\n",
        " \n",
        "for _ in range(100):\n",
        "    r = random.random()\n",
        "    accumulator = .0\n",
        " \n",
        "    for word, freq in counts.items():\n",
        "        accumulator += freq\n",
        " \n",
        "        if accumulator >= r:\n",
        "            text.append(word)\n",
        "            break\n",
        " \n",
        "print(' '.join(text))\n",
        " "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "moored effectively offered ENCOR . Capital alleged program due in that another net ended SFTPF have an 152 are - gas allowed . 200 , not 9 offer at . in 104 because ; seismic Belgium Standard the JAPAN surplus pct , its comment - expected gain General securities the cts at vs free France Avg OPERATIONS offer the British > consumer as mandatory > the of 6 the formerly investment , / survey vs said pct On through 5 crop 7 , an also . ended the 5 W of / Hong is controversial revised profit disillusionment and 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzlH_2nW-gyQ",
        "outputId": "30ae910e-76f5-4c17-84df-35288df1f33f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The probability of the text\n",
        "\n",
        "print(reduce(mul, [counts[w] for w in text], 1.0))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.5735802169330975e-304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDctxYLcBooF"
      },
      "source": [
        "Compare the given probability above to the one you've previously guessed.\n",
        "\n",
        "Try changing the number of words and compare the results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftXjYNnKCMaT"
      },
      "source": [
        "# n-gram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pIOV53YCPrb"
      },
      "source": [
        "Now let's construct a n-grams language model from the text.\n",
        "\n",
        "nltk already has functions for n-grams such as: `bigrams` & `trigrams`.\n",
        "\n",
        "Let's use them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZZzik2A_SZR",
        "outputId": "4f78e9c0-d63e-4d44-abf6-52682831f2d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "first_sentence = reuters.sents()[0]\n",
        "print(first_sentence)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifXUjkxJ_518",
        "outputId": "f81f51f5-29a4-49f9-a555-6ef152dbfdb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the bigrams\n",
        "print(list(bigrams(first_sentence)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWRQ3iJwASBT",
        "outputId": "2c89b27e-b339-4fdb-ab86-f45a248ac1b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the padded bigrams\n",
        "print(list(bigrams(first_sentence, pad_left=True, pad_right=True)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(None, 'ASIAN'), ('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.'), ('.', None)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow0OeOo3AS7R",
        "outputId": "ecded1ee-b548-48ec-a0b7-07cb56664827",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the trigrams\n",
        "print(list(trigrams(first_sentence)))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxnGokXfAT5N",
        "outputId": "87389dea-ab40-471c-a72e-bce50df672c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the padded trigrams\n",
        "print(list(trigrams(first_sentence, pad_left=True, pad_right=True)))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(None, None, 'ASIAN'), (None, 'ASIAN', 'EXPORTERS'), ('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.'), ('said', '.', None), ('.', None, None)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmTKkqbJCjAQ"
      },
      "source": [
        "Let's use the trigrams on the Reuters corpus.\n",
        "\n",
        "We start by counting the occurences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CGdcS-wAVX0",
        "outputId": "598b9fe8-7d38-4ff4-8dd3-75a4fba3cfac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        " \n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        " \n",
        " \n",
        "print(model[\"what\", \"the\"][\"economists\"])\n",
        "print(model[\"what\", \"the\"][\"nonexistingword\"])\n",
        "print(model[None, None][\"The\"])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "0\n",
            "8839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_L2wmE4DLLd"
      },
      "source": [
        "And then converting it into frequencies or probabilities, by deviding these occurences by the total number of occurences of the first 2 words of our trigrams:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFVttrieC8RF"
      },
      "source": [
        "# Let's transform the counts to probabilities\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5AlcTmgJs0T",
        "outputId": "af79c47c-174a-4019-e760-466c0b0991f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(model[\"what\", \"the\"][\"economists\"])\n",
        "print(model[\"what\", \"the\"][\"nonexistingword\"])\n",
        "print(model[None, None][\"The\"])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.043478260869565216\n",
            "0.0\n",
            "0.16154324146501936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8F8Y2lVDqLV"
      },
      "source": [
        "And now we're ready to try it out.\n",
        "\n",
        "Let's generate a random sentence again, but this time, we'll use our trigram model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhkojQA3Di_p",
        "outputId": "4d8cb97e-0196-449a-f2e2-5f4d7199e7aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text = [None, None]\n",
        "prob = 1.0 \n",
        " \n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "    r = random.random()\n",
        "    accumulator = .0\n",
        " \n",
        "    for word in model[tuple(text[-2:])].keys():\n",
        "        accumulator += model[tuple(text[-2:])][word]\n",
        " \n",
        "        if accumulator >= r:\n",
        "            prob *= model[tuple(text[-2:])][word] \n",
        "            text.append(word)\n",
        "            break\n",
        " \n",
        "    if text[-2:] == [None, None]:\n",
        "        sentence_finished = True\n",
        " \n",
        "print(f'Probability of text={prob}')\n",
        "print(' '.join([t for t in text if t]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability of text=8.721329568608191e-20\n",
            "\" The future of the merger announced last September , compared with the threat of competition .\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMQG202UD0aD"
      },
      "source": [
        "Try running this cell several times, and compare the output sentences to the previous random ones.\n",
        "\n",
        "> Which one looks better to you?\n",
        " \n",
        "> Can you explain why?\n",
        "\n",
        "\n",
        "Note that we have not used here complicated RNN or LSTM, and still managed to generate reasonable sentences, using only simple probability, and counting words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2CyR3qoFt1G"
      },
      "source": [
        "---\n",
        "\n",
        "Your Turn:\n",
        "- Optional: use your own corpus\n",
        "- Create a function that parse the text into 4-grams\n",
        "- Train a language model using the 4-grmams and\n",
        "- Generate few sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31Ht7Wu44mY4"
      },
      "source": [
        "Q: How can you use your language model as a spelling or grammar checker?\n",
        "\n",
        "Q2: What steps are needed to create a spelling or grammar correction suggestion using your language model? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKlyMas_4kBh"
      },
      "source": [
        "\n",
        "Task 2:\n",
        "- For those of you who have finished it quickly, can you improve the generation even further? Hint: currently the comparisson is above a random threshold...\n",
        "\n",
        "> Do they look better? Do they make more sense?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZySTIWy3Dn14"
      },
      "source": [
        "# Get the 4-grams\n",
        "# nltk.ngrams(first_sentence,n=4)\n",
        "# print(list(fourgrams(first_sentence)))\n"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}