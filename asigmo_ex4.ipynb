{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "asigmo_ex4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mahnazshamissa/Python/blob/main/asigmo_ex4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLvl9qZpRR1A"
      },
      "source": [
        "# Exercise 4\n",
        "\n",
        "\n",
        "Per group create a teams-meeting where one person opens the colab notebook and shares the screen (preferably a different person than yesterday)\n",
        "\n",
        "1.   Change the data source from \"frankenstein.txt\" to \"dracula.txt\", \n",
        "2.   Train the network for 10 epochs and test it on the 3 example sentences\n",
        "3.   Write down 3 more example sentences and test the model on these as well\n",
        "4.   Train the LSTM for 50 more epochs and run the completion again. What do you observe?\n",
        "4.   Add another LSTM layer to our RNN model\n",
        "5.   Train the 2-layer LSTM for 10 epochs and see how the sentence completion has changed\n",
        "6.   Change the \"dracula.txt\" to \"trump.txt\", which contains Tweets from Donald Trump from 2019 and 2020.\n",
        "7.   Train a single-layer LSTM on the Tweet's from Trump. If you run out of memory while preprocessing the data, change ```step = 3``` to ```step = 10```.\n",
        "8.   Run the completion on the six sentences from before again. What do you observe?\n",
        "\n",
        "\n",
        "\n",
        "**Question**\n",
        "The models make very few spelling errors and few grammar error, but the content is mostly nonsense. Why do you think are the error types so different?\n",
        "\n",
        "We will meet in the general channel at 12:10!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A4xscds3KtQ"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTLXcbOl3n2E"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He7DevTX5gcs"
      },
      "source": [
        "# Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhAlKCjm5f0b"
      },
      "source": [
        "filename = \"frankenstein.txt\"\n",
        "path = tf.keras.utils.get_file(\n",
        "    filename, origin=f\"https://pub.ist.ac.at/~mlechner/datasets/{filename}\"\n",
        ")\n",
        "with open(path, encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()           # Make lowercase \n",
        "text = text.replace(\"\\n\", \" \")        # Remove line-breaks & newlines\n",
        "print(\"Corpus length:\", len(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtFFQXZB5pfj"
      },
      "source": [
        "# Print the first 100 characters of the file we downloaded\n",
        "print(\"First 100 characters: \",text[:100])\n",
        "print(\"Last 100 characters : \",text[-100:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ6Kxxoo6Pg1"
      },
      "source": [
        "# Define input features and output labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFTI5-543yKi"
      },
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(\"Total chars:\", len(chars))\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "maxlen = 50\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i : i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print(\"Number of sequences:\", len(sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z75uMG-35Ihz"
      },
      "source": [
        "print(\"First sentence:\")\n",
        "print(sentences[0])\n",
        "print(\"First label: \")\n",
        "print(next_chars[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eIkuFt65aQF"
      },
      "source": [
        "# Transforming text into numerical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJP69sDV5VGj"
      },
      "source": [
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros(len(sentences), dtype=np.int32)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i] = char_indices[next_chars[i]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KbhiVJ140xE"
      },
      "source": [
        "# First training samples\n",
        "print(x[0])\n",
        "print(\"label: \")\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwfhq3pnEBit"
      },
      "source": [
        "# Splitting training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW4OyUE4DZS9"
      },
      "source": [
        "# We can define our training and validation dataset as simple numpy arrays\n",
        "split_index = int(0.2*x.shape[0])\n",
        "valid_x, valid_y = x[-split_index:],y[-split_index:]\n",
        "train_x, train_y = x[:-split_index],y[:-split_index]\n",
        "print(\"valid_x.shape\",valid_x.shape)\n",
        "print(\"valid_y.shape\",valid_y.shape)\n",
        "print(\"train_x.shape\",train_x.shape)\n",
        "print(\"train_y.shape\",train_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCurnl9YOAM1"
      },
      "source": [
        "# Defining our Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se-1ndKD4E4i"
      },
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(maxlen, len(chars))),\n",
        "        tf.keras.layers.LSTM(128),\n",
        "        tf.keras.layers.Dense(len(chars), activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,metrics=[\"sparse_categorical_accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ0PMEJlRDyd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1NLlj4z4bkK"
      },
      "source": [
        "training_log = model.fit(train_x, train_y, batch_size=64, epochs=10, validation_data=(valid_x,valid_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-F2scl-RFjz"
      },
      "source": [
        "# Let's try out our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGqx_9q-A6kb"
      },
      "source": [
        "def sample(preds, temperature=0.5):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\")\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "    \n",
        "def complete_sentence(sentence):\n",
        "  if len(sentence) < maxlen:\n",
        "    raise ValueError(f\"Sentence must have at least {maxlen} characters, but provided sentence had only {sentence}\")\n",
        "  if len(sentence) > maxlen:\n",
        "    sentence = sentence[-maxlen:]\n",
        "  generated = \"\"\n",
        "  print(f\"Continuing sentence '{sentence}' ...\")\n",
        "\n",
        "  for i in range(200):\n",
        "      x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "      for t, char in enumerate(sentence):\n",
        "          x_pred[0, t, char_indices[char]] = 1.0\n",
        "      preds = model.predict(x_pred, verbose=0)[0]\n",
        "      next_index = sample(preds)\n",
        "      next_char = indices_char[next_index]\n",
        "      sentence = sentence[1:] + next_char\n",
        "      generated += next_char\n",
        "\n",
        "  print(\"... \", generated)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZAW4zpVBgzj"
      },
      "source": [
        "complete_sentence(\"this is an example to show how the machine would continue\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQBfHwCJUmh2"
      },
      "source": [
        "### TODO: Add 3 more sentences to complete. Think about what types of sentences would be interesting to test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtcRdawJPE9s"
      },
      "source": [
        "# Biased data -> biased model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ij4obo3uPKOq"
      },
      "source": [
        "complete_sentence(\"    was the thiefs answer, to which the man replied \")\n",
        "complete_sentence(\"    was the thiefs answer, to which the woman replied \")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}